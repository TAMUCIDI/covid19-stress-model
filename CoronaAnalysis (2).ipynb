{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "df = pd.concat(map(pd.read_csv, glob.glob('./coronavirus-covid19-tweets/*2 Coronavirus Tweets.CSV')))\n",
    "\n",
    "# Remove Tweets in other languages\n",
    "df = df[df[\"lang\"]==\"en\"]\n",
    "\n",
    "# df_processed = df[['user_id','text']].groupby('user_id', as_index = False).agg({'text': 'sum'})\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    # convert text to lower-case\n",
    "    nopunc = nopunc.lower()\n",
    "    # remove URLs\n",
    "    nopunc = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', nopunc)\n",
    "    nopunc = re.sub(r'http\\S+', '', nopunc)\n",
    "    # remove usernames\n",
    "    nopunc = re.sub('@[^\\s]+', '', nopunc)\n",
    "    # remove the # in #hashtag\n",
    "    nopunc = re.sub(r'#([^\\s]+)', r'\\1', nopunc)\n",
    "    #remove numbers\n",
    "    nopunc = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\",\" \", nopunc)\n",
    "    nopunc = re.sub(\"\\d\", \"\", nopunc)\n",
    "    # remove repeated characters\n",
    "    nopunc = re.sub('(corona|covid|virus)', '', nopunc)\n",
    "    nopunc = word_tokenize(nopunc)\n",
    "#     return nopunc\n",
    "    # remove stopwords from final word list\n",
    "    [word for word in nopunc if word not in stopwords.words('english')]\n",
    "    text = ' '.join([str(elem) for elem in nopunc]) + \"\\n\"\n",
    "    return text\n",
    "#     f.write(' '.join([str(elem) for elem in nopunc]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1870846\n"
     ]
    }
   ],
   "source": [
    "x += len(df)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266973"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet = \"RT @marcobonzanini: just an example! #coronavirus :D http://example.com #NLP 1000 p coron6a\"\n",
    "# print(preprocess_tweet(tweet))\n",
    "# df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tweets1.txt', 'w', encoding=\"utf8\") as f:\n",
    "#     df.text.map(lambda x:preprocess_tweet(x, f))\n",
    "df['clean_text'] = df.text.map(lambda x:preprocess_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>reply_to_status_id</th>\n",
       "      <th>reply_to_user_id</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>...</th>\n",
       "      <th>country_code</th>\n",
       "      <th>place_full_name</th>\n",
       "      <th>place_type</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>account_lang</th>\n",
       "      <th>account_created_at</th>\n",
       "      <th>verified</th>\n",
       "      <th>lang</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1237891054600241152</td>\n",
       "      <td>3520752864</td>\n",
       "      <td>2020-03-12T00:00:00Z</td>\n",
       "      <td>Tokyo_gov</td>\n",
       "      <td>Watch this newly released video featuring a me...</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118670</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-11T00:05:21Z</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>watch this newly released video featuring a me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1237891054399115264</td>\n",
       "      <td>1056850669</td>\n",
       "      <td>2020-03-12T00:00:00Z</td>\n",
       "      <td>airnewsalerts</td>\n",
       "      <td>60 positive cases of #Covid19 in India, says H...</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2236114</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-03T04:15:45Z</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>positive cases of in india says health ministry\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1237891053988073473</td>\n",
       "      <td>794162328</td>\n",
       "      <td>2020-03-12T00:00:00Z</td>\n",
       "      <td>StatesideRadio</td>\n",
       "      <td>UM medical historian @HowardMarkel says he doe...</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3786</td>\n",
       "      <td>412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-08-31T16:08:32Z</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>um medical historian howardmarkel says he does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1237891053686075392</td>\n",
       "      <td>38489678</td>\n",
       "      <td>2020-03-12T00:00:00Z</td>\n",
       "      <td>JAMA_current</td>\n",
       "      <td>This JAMA Insights article reviews care for th...</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>335352</td>\n",
       "      <td>805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-05-07T18:45:39Z</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>this jama insights article reviews care for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1237891053627162625</td>\n",
       "      <td>824565311437410305</td>\n",
       "      <td>2020-03-12T00:00:00Z</td>\n",
       "      <td>ViralTabNews</td>\n",
       "      <td>He beat the #coronavirus at age of101 \\nhttps:...</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "      <td>529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-26T10:31:04Z</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>he beat the at age of\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              status_id             user_id            created_at  \\\n",
       "7   1237891054600241152          3520752864  2020-03-12T00:00:00Z   \n",
       "8   1237891054399115264          1056850669  2020-03-12T00:00:00Z   \n",
       "9   1237891053988073473           794162328  2020-03-12T00:00:00Z   \n",
       "11  1237891053686075392            38489678  2020-03-12T00:00:00Z   \n",
       "12  1237891053627162625  824565311437410305  2020-03-12T00:00:00Z   \n",
       "\n",
       "       screen_name                                               text  \\\n",
       "7        Tokyo_gov  Watch this newly released video featuring a me...   \n",
       "8    airnewsalerts  60 positive cases of #Covid19 in India, says H...   \n",
       "9   StatesideRadio  UM medical historian @HowardMarkel says he doe...   \n",
       "11    JAMA_current  This JAMA Insights article reviews care for th...   \n",
       "12    ViralTabNews  He beat the #coronavirus at age of101 \\nhttps:...   \n",
       "\n",
       "       source  reply_to_status_id  reply_to_user_id reply_to_screen_name  \\\n",
       "7   TweetDeck                 NaN               NaN                  NaN   \n",
       "8   TweetDeck                 NaN               NaN                  NaN   \n",
       "9   TweetDeck                 NaN               NaN                  NaN   \n",
       "11   Sprinklr                 NaN               NaN                  NaN   \n",
       "12  TweetDeck                 NaN               NaN                  NaN   \n",
       "\n",
       "    is_quote  ...  country_code  place_full_name  place_type followers_count  \\\n",
       "7      False  ...           NaN              NaN         NaN          118670   \n",
       "8      False  ...           NaN              NaN         NaN         2236114   \n",
       "9      False  ...           NaN              NaN         NaN            3786   \n",
       "11     False  ...           NaN              NaN         NaN          335352   \n",
       "12     False  ...           NaN              NaN         NaN              86   \n",
       "\n",
       "   friends_count account_lang    account_created_at  verified  lang  \\\n",
       "7             16          NaN  2015-09-11T00:05:21Z      True    en   \n",
       "8             13          NaN  2013-01-03T04:15:45Z      True    en   \n",
       "9            412          NaN  2012-08-31T16:08:32Z     False    en   \n",
       "11           805          NaN  2009-05-07T18:45:39Z      True    en   \n",
       "12           529          NaN  2017-01-26T10:31:04Z     False    en   \n",
       "\n",
       "                                           clean_text  \n",
       "7   watch this newly released video featuring a me...  \n",
       "8   positive cases of in india says health ministry\\n  \n",
       "9   um medical historian howardmarkel says he does...  \n",
       "11  this jama insights article reviews care for th...  \n",
       "12                            he beat the at age of\\n  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'ProcessedTweets.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('basilisk-master/ProcessedTweetsTextOnly.txt', 'w', encoding=\"utf8\") as f:\n",
    "    for val in df['clean_text'].values:\n",
    "        f.write(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r'basilisk-master/ProcessedTweetsTextOnly.csv', index = False, columns=['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "inputFolder = \"./basilisk-master/out_stressed2/\"\n",
    "tokenizer = TweetTokenizer()\n",
    "# punct = list(string.punctuation)\n",
    "# stopwords_additional = ['coronavirus', 'covid19', 'covid-19', 'covid', 'covid_19', '19', 'corona']\n",
    "# stopwords_list = stopwords.words('english') + punct + ['rt','via','...','â€¦','â€™','â€”','â€”:','â€œ'] + stopwords_additional\n",
    "# print(stopwords_list)\n",
    "\n",
    "\n",
    "def read_words(infile):\n",
    "    words = []\n",
    "    with open(infile, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            words.append(line.strip())\n",
    "    return words\n",
    "\n",
    "stress_words = read_words(inputFolder + \"STRESSED-NS-diffScore.lexicon\")            \n",
    "relax_words = read_words(inputFolder + \"NOT_STRESSED-NS-diffScore.lexicon\")\n",
    "# stress_words.extend(read_words(inputFolder + \"sample-data/seeds/STRESSED\"))\n",
    "\n",
    "# stress_words = read_words(inputFolder + \"sample-data/seeds/STRESSED\")            \n",
    "# relax_words = read_words(inputFolder + \"sample-data/seeds/NOT_STRESSED\")\n",
    "\n",
    "# stress_words = ['stress']\n",
    "# relax_words = ['relax']\n",
    "\n",
    "# safe_words = read_words(safe_words_file)            \n",
    "# hazard_words = read_words(hazard_words_file)\n",
    "\n",
    "def process_tweets(text): \n",
    "#     count_safe = 0\n",
    "#     count_hazard = 0\n",
    "    count_stress = 0\n",
    "    count_relax = 0\n",
    "    text = text.lower()\n",
    "#     count_stress = text.count('stress')\n",
    "#     count_relax = text.count('relax')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "#     tokens = [tok for tok in tokens if tok not in stopwords_list and not tok.isdigit()]\n",
    "#     for tok in tokens:\n",
    "#         if tok in stress_words:\n",
    "#             count_stress +=1\n",
    "#         if tok in relax_words:\n",
    "#             count_relax +=1\n",
    "#     if count_stress > count_relax:\n",
    "#         return 1\n",
    "#     if count_relax > count_stress:\n",
    "#         return 2\n",
    "#     return 0\n",
    "#     f.write(' '.join([str(elem) for elem in tokens]) + \"\\n\")\n",
    "#     print(tokens)\n",
    "#     return count_safe, count_hazard\n",
    "\n",
    "    for tok in tokens:\n",
    "        if tok in stress_words:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def process_tweets_safe(text): \n",
    "    count_safe = 0\n",
    "    count_hazard = 0\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for tok in tokens:\n",
    "        if tok in safe_words:\n",
    "            count_safe +=1\n",
    "    return count_safe\n",
    "\n",
    "def process_tweets_hazard(text): \n",
    "    count_safe = 0\n",
    "    count_hazard = 0\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for tok in tokens:\n",
    "        if tok in hazard_words:\n",
    "            count_hazard +=1\n",
    "    return count_hazard\n",
    "\n",
    "# for text in df_temp['clean_text'][:100]:\n",
    "# #     text = text.to_string(index=False)\n",
    "#     count_safe, count_hazard = process_tweets(text)\n",
    "# #     print(\"Safe count: {0}, Hazard count: {1}\".format(count_safe, count_hazard))\n",
    "    \n",
    "# df_temp[\"count_safe\"] = df_temp.clean_text.map(lambda x:process_tweets_safe(x))\n",
    "# df_temp[\"count_hazard\"] = df_temp.clean_text.map(lambda x:process_tweets_hazard(x))\n",
    "\n",
    "# df_kmeans1 = df_temp[[\"count_safe\",\"count_hazard\"]]\n",
    "# df_kmeans1.head()\n",
    "# keyword_count_array = np.array(df_kmeans1)\n",
    "# # px.scatter(df, x=\"Sentiment\", y=\"Subjectivity\",color=\"cluster_sentiment\")\n",
    "# n_cluster = 5\n",
    "# pred = KMeans(n_clusters=n_cluster).fit_predict(keyword_count_array)\n",
    "\n",
    "# # df_temp[['clean_text', 'count_safe', 'count_hazard']].head()\n",
    "# df_temp[\"cluster_keyword_count\"] = pred\n",
    "# df_temp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "inputFolder = \"./basilisk-master/out_stressed2/\"\n",
    "tokenizer = TweetTokenizer()\n",
    "# punct = list(string.punctuation)\n",
    "# stopwords_additional = ['coronavirus', 'covid19', 'covid-19', 'covid', 'covid_19', '19', 'corona']\n",
    "# stopwords_list = stopwords.words('english') + punct + ['rt','via','...','â€¦','â€™','â€”','â€”:','â€œ'] + stopwords_additional\n",
    "# print(stopwords_list)\n",
    "\n",
    "\n",
    "def read_words(infile):\n",
    "    words = []\n",
    "    with open(infile, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            words.append(line.strip())\n",
    "    return words\n",
    "\n",
    "stress_words = read_words(inputFolder + \"STRESSED-NS-diffScore.lexicon\")            \n",
    "relax_words = read_words(inputFolder + \"NOT_STRESSED-NS-diffScore.lexicon\")\n",
    "# stress_words.extend(read_words(inputFolder + \"sample-data/seeds/STRESSED\"))\n",
    "\n",
    "# stress_words = read_words(inputFolder + \"sample-data/seeds/STRESSED\")            \n",
    "# relax_words = read_words(inputFolder + \"sample-data/seeds/NOT_STRESSED\")\n",
    "\n",
    "# stress_words = ['stress']\n",
    "# relax_words = ['relax']\n",
    "\n",
    "# safe_words = read_words(safe_words_file)            \n",
    "# hazard_words = read_words(hazard_words_file)\n",
    "\n",
    "def process_tweets(text): \n",
    "#     count_safe = 0\n",
    "#     count_hazard = 0\n",
    "    count_stress = 0\n",
    "    count_relax = 0\n",
    "    text = text.lower()\n",
    "#     count_stress = text.count('stress')\n",
    "#     count_relax = text.count('relax')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "#     tokens = [tok for tok in tokens if tok not in stopwords_list and not tok.isdigit()]\n",
    "    for tok in tokens:\n",
    "        if tok in stress_words:\n",
    "            count_stress +=1\n",
    "        if tok in relax_words:\n",
    "            count_relax +=1\n",
    "    if count_stress > count_relax:\n",
    "        return 1\n",
    "    if count_relax > count_stress:\n",
    "        return 2\n",
    "    return 0\n",
    "#     f.write(' '.join([str(elem) for elem in tokens]) + \"\\n\")\n",
    "#     print(tokens)\n",
    "#     return count_safe, count_hazard\n",
    "\n",
    "#     for tok in tokens:\n",
    "#         if tok in stress_words:\n",
    "#             return 1\n",
    "#     return 0\n",
    "\n",
    "\n",
    "def process_tweets_safe(text): \n",
    "    count_safe = 0\n",
    "    count_hazard = 0\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for tok in tokens:\n",
    "        if tok in safe_words:\n",
    "            count_safe +=1\n",
    "    return count_safe\n",
    "\n",
    "def process_tweets_hazard(text): \n",
    "    count_safe = 0\n",
    "    count_hazard = 0\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for tok in tokens:\n",
    "        if tok in hazard_words:\n",
    "            count_hazard +=1\n",
    "    return count_hazard\n",
    "\n",
    "# for text in df_temp['clean_text'][:100]:\n",
    "# #     text = text.to_string(index=False)\n",
    "#     count_safe, count_hazard = process_tweets(text)\n",
    "# #     print(\"Safe count: {0}, Hazard count: {1}\".format(count_safe, count_hazard))\n",
    "    \n",
    "# df_temp[\"count_safe\"] = df_temp.clean_text.map(lambda x:process_tweets_safe(x))\n",
    "# df_temp[\"count_hazard\"] = df_temp.clean_text.map(lambda x:process_tweets_hazard(x))\n",
    "\n",
    "# df_kmeans1 = df_temp[[\"count_safe\",\"count_hazard\"]]\n",
    "# df_kmeans1.head()\n",
    "# keyword_count_array = np.array(df_kmeans1)\n",
    "# # px.scatter(df, x=\"Sentiment\", y=\"Subjectivity\",color=\"cluster_sentiment\")\n",
    "# n_cluster = 5\n",
    "# pred = KMeans(n_clusters=n_cluster).fit_predict(keyword_count_array)\n",
    "\n",
    "# # df_temp[['clean_text', 'count_safe', 'count_hazard']].head()\n",
    "# df_temp[\"cluster_keyword_count\"] = pred\n",
    "# df_temp.head()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stress_label'] = df.clean_text.map(lambda x:process_tweets(x))\n",
    "# df['stress_label2'] = df.clean_text.map(lambda x:process_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r'ProcessedTweetsWithStressLabels.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['stress_label_new'] = df.clean_text.map(lambda x:process_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['clean_text', 'stress_label']].head()\n",
    "# print(df[df['stress_label']==2][['clean_text', 'stress_label']]['clean_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hchau\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hchau\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "X_train_df = df[df['stress_label']>=1]\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = tf.ConfigProto(allow_soft_placement=True )\n",
    "# cfg.gpu_options.allow_growth = True\n",
    "# with tf.Session(config=cfg) as session:\n",
    "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#     message_embeddings = session.run(embed(X_train_df['clean_text'].values))\n",
    "# #     message_embeddings = session.run(embed(list(user_tweets.values())))\n",
    "# print(message_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk(df):\n",
    "    data = df[\"clean_text\"].tolist()\n",
    "    chunks_size = 500\n",
    "    chunks = [data[x : x + chunks_size] for x in range(0, len(data), chunks_size)]\n",
    "    print(len(chunks))\n",
    "    return chunks\n",
    "\n",
    "# chunks = get_chunk(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "cfg = tf.compat.v1.ConfigProto(allow_soft_placement=True )\n",
    "cfg.gpu_options.allow_growth = True\n",
    "\n",
    "def get_embedding(chunks):\n",
    "    message_embeddings = []\n",
    "    with tf.Session(config=cfg) as session:\n",
    "        for index, ck in enumerate(chunks):\n",
    "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "            # if index == 1:\n",
    "            #   break\n",
    "#             message_embeddings.append(session.run(embed(ck)))\n",
    "            message_embeddings.extend(session.run(embed(ck)))\n",
    "            print(\"finished \", index)\n",
    "            # message_embeddings = session.run(embed(list(user_tweets.values())))\n",
    "    return message_embeddings\n",
    "# message_embeddings = get_embedding(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235966\n"
     ]
    }
   ],
   "source": [
    "print(len(df[df['stress_label']==1]))\n",
    "print(len(df[df['stress_label']==2]))\n",
    "# print(len(df[df['stress_label2']==2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_stress = get_chunk(df[df['stress_label2']==1][:10000])\n",
    "chunks_relax = get_chunk(df[df['stress_label2']==0][:10000])\n",
    "stress_text_embeddings = get_embedding(chunks_stress)\n",
    "relax_text_embeddings = get_embedding(chunks_relax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = []\n",
    "for embedding in stress_text_embeddings:  \n",
    "    X.append(embedding)\n",
    "for embedding in relax_text_embeddings:  \n",
    "    X.append(embedding)\n",
    "# relax_text_embeddings.shape\n",
    "len(X)\n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text_list):\n",
    "    with tf.Session(config=cfg) as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        return session.run(embed(text_list))\n",
    "\n",
    "# stress_text_embeddings = get_text_embedding(df[df['stress_label_new']==1]['clean_text'].values)\n",
    "# relax_text_embeddings = get_text_embedding(df[df['stress_label_new']==2]['clean_text'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stress_text_embeddings.shape)\n",
    "# print(relax_text_embeddings.shape)\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# X = np.concatenate((stress_text_embeddings[:250], relax_text_embeddings[:250]), axis=0)\n",
    "# X = stress_text_embeddings[:250]\n",
    "# np.append(X, relax_text_embeddings[:250])\n",
    "# y = np.ones((250,), dtype=int)\n",
    "# np.append(y, np.zeros((250,), dtype=int))\n",
    "ones = np.ones((10000,), dtype=int)\n",
    "zeros = np.zeros((10000,), dtype=int)\n",
    "y = np.concatenate((ones,zeros), axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# X_val = np.concatenate((stress_text_embeddings, relax_text_embeddings), axis=0)\n",
    "# y_val = np.concatenate((np.ones((len(stress_text_embeddings),), dtype=int),np.zeros((len(relax_text_embeddings),), dtype=int)), axis=0)\n",
    "# c = list(zip(X_val, y_val))\n",
    "\n",
    "# random.shuffle(c)\n",
    "\n",
    "# X_val, y_val = zip(*c)\n",
    "# X_val, _, y_val, _ = train_test_split(X_val, y_val, test_size=0, random_state=42, shuffle=True)\n",
    "\n",
    "# stress_text_embeddings_labeled = [(embedding, 1) for embedding in stress_text_embeddings]\n",
    "# relax_text_embeddings_labeled = [(embedding, 1) for embedding in relax_text_embeddings]\n",
    "# text_embeddings_labeled = []\n",
    "# text_embeddings_labeled.extend(stress_text_embeddings_labeled)\n",
    "# text_embeddings_labeled.extend(relax_text_embeddings_labeled)\n",
    "# # random.shuffle(text_embeddings_labeled)\n",
    "# print(text_embeddings_labeled[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC(kernel='linear', C=1,gamma='auto')\n",
    "svc.fit(X_train, y_train)\n",
    "print(\"Accuracy for train: \", svc.score(X_train, y_train))\n",
    "print(\"Accuracy for test: \", svc.score(X_test, y_test))\n",
    "# print(\"Accuracy for stressed: \", svc.score(stress_text_embeddings, np.ones((len(stress_text_embeddings),), dtype=int)))\n",
    "# print(\"Accuracy for relaxed: \", svc.score(relax_text_embeddings, np.zeros((len(relax_text_embeddings),), dtype=int)))\n",
    "# print(\"Accuracy for mixed: \", svc.score(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# svclassifier = SVC(C=10, probability=True, gamma='scale', kernel='linear')\n",
    "# X_train, X_test, y_train, y_test =   cross_validation.train_test_split(iris.data,   iris.target, test_size=0.10, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC(kernel='rbf', C=1,gamma='auto')\n",
    "svc.fit(X_train, y_train)\n",
    "print(\"Accuracy for train: \", svc.score(X_train, y_train))\n",
    "print(\"Accuracy for test: \", svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Accuracy for train: \", lr.score(X_train, y_train))\n",
    "print(\"Accuracy for test: \", lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(512,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=512)\n",
    "results = model.evaluate(X_test, y_test)\n",
    "results\n",
    "\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "# model.add(layers.Dense(16, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='rmsprop',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# history = model.fit(partial_x_train,\n",
    "#                     partial_y_train,\n",
    "#                     epochs=20,\n",
    "#                     batch_size=512,\n",
    "#                     validation_data=(x_val, y_val))\n",
    "# history_dict = history.history\n",
    "# history_dict.keys()\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# acc = history.history['acc']\n",
    "# val_acc = history.history['val_acc']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# # \"bo\" is for \"blue dot\"\n",
    "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# # b is for \"solid blue line\"\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(get_text_embedding(df['clean_text'].values))\n",
    "# chunks = get_chunk(df[:50000])\n",
    "chunks = get_chunk(df)\n",
    "\n",
    "text_embeddings = get_embedding(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tweets_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_labeled[df_labeled['predicted_prob']<0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(text_embeddings)\n",
    "X.shape\n",
    "# y_pred = model.predict(text_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(get_text_embedding(df[0]['clean_text'].values))\n",
    "# y_pred.shape\n",
    "y_pred = model.predict(X)\n",
    "y_list = [pred[0] for pred in y_pred]\n",
    "print(len(y_list))\n",
    "df_labeled = df[:50000]\n",
    "print(len(df_labeled))\n",
    "df_labeled['predicted_prob'] = y_list\n",
    "df_labeled.head()\n",
    "print(df_labeled[df_labeled['predicted_prob']>0.5]['clean_text'].values[:50])\n",
    "# y_list_stress = [pred[0] for pred in y_pred if pred[0]>]\n",
    "# y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter \n",
    "# # wordcloud in python\n",
    "# from wordcloud import WordCloud, STOPWORDS \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import re \n",
    "# import string\n",
    "# import nltk # preprocessing text\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# wordcloud = WordCloud(width=1600, height=800, random_state=1, max_words=100,colormap=\"Paired\", background_color='black',)\n",
    "# wordcloud.generate(str(df_labeled[df_labeled['predicted_prob']>0.5]['clean_text']))\n",
    "# # declare our figure \n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.title(\"Top words - tweets\", fontsize=20,color='black')\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout(pad=10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_list = []\n",
    "for prob in y_list:\n",
    "    if prob >= 0.5:\n",
    "        y_labels_list.append(1)\n",
    "    else:\n",
    "        y_labels_list.append(0)\n",
    "df_labeled['predicted_label'] = y_labels_list\n",
    "\n",
    "df_labeled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_2 = LogisticRegression(random_state=0)\n",
    "lr_2.fit(X, y_labels_list)\n",
    "print(\"Accuracy for train: \", lr.score(X, y_labels_list))\n",
    "# print(\"Accuracy for test: \", lr.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tweets_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_2.predict(embed(\"I am sad\")\n",
    "# y_pred2 = lr_2.predict_proba(get_text_embedding([\"There is too much work and no time\"]))\n",
    "y_pred2 = lr_2.predict(X)\n",
    "df_labeled['lr_label'] = y_pred2.tolist()\n",
    "print(len(df_labeled[df_labeled['lr_label']==1]))\n",
    "print(df_labeled[df_labeled['lr_label']==1]['text'].values)\n",
    "\n",
    "# print(y_pred2.tolist())\n",
    "# print(df_labeled['clean_text'].values[:21])\n",
    "# df_labeled2 = df_labeled[:50000]\n",
    "# print(len(df_labeled))\n",
    "# df_labeled['predicted_prob'] = y_list\n",
    "# df_labeled.head()\n",
    "# print(df_labeled[df_labeled['predicted_prob']>0.5]['clean_text'].values[:50])\n",
    "# y_list2 = [pred[0] for pred in y_pred2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stress_classified_tweets.txs', 'w', encoding=\"utf8\") as f:\n",
    "    for val in df_labeled[df_labeled['lr_label']==1]['text']:\n",
    "        f.write(val +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['clean_text'].values[:20])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "print(len(X_train))\n",
    "print(\"Accuracy for train: \", gnb.score(X_train, y_train))\n",
    "print(\"Accuracy for test: \", gnb.score(X_test, y_test))\n",
    "# y_pred = gnb.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df[:50000].copy()\n",
    "df_processed = df_processed[df_processed[\"lang\"] == \"en\"]\n",
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['pro_text'] = df_processed.apply(lambda row: preprocess_tweet(row['text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# dimensions = 200\n",
    "\n",
    "# # Get embeddings\n",
    "# pretrained_vectors = {}\n",
    "# f = open('./glove.twitter.27B/glove.twitter.27B.200d.txt', encoding=\"utf8\")\n",
    "# for line in f:\n",
    "#     values = line.rstrip().rsplit(' ', dimensions)\n",
    "#     word = values[0]\n",
    "#     probabilities = np.asarray(values[1:], dtype='float32')\n",
    "#     pretrained_vectors[word] = probabilities\n",
    "# f.close()\n",
    "\n",
    "# print('Found {} word vectors.'.format(len(pretrained_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pretrained_vectors['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_processed[['user_id','pro_text']].groupby('user_id', as_index = False).agg({'pro_text': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['pro_text']=[\" \".join(review) for review in df_processed['pro_text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    max_df = 0.95,\n",
    "    max_features = 8000,\n",
    "    stop_words = 'english'\n",
    ")\n",
    "tfidf.fit(df_processed.pro_text)\n",
    "text = tfidf.transform(df_processed.pro_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "def find_optimal_clusters(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')\n",
    "    \n",
    "find_optimal_clusters(text, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_pca(data, labels):\n",
    "    max_label = max(labels)\n",
    "    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)\n",
    "    \n",
    "    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())\n",
    "    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))\n",
    "    \n",
    "    \n",
    "    idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)\n",
    "    label_subset = labels[max_items]\n",
    "    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
    "    \n",
    "    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n",
    "    ax[0].set_title('PCA Cluster Plot')\n",
    "    \n",
    "    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n",
    "    ax[1].set_title('TSNE Cluster Plot')\n",
    "    \n",
    "clusters = MiniBatchKMeans(n_clusters=5, init_size=1024, batch_size=2048, random_state=20).fit_predict(text)\n",
    "plot_tsne_pca(text, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(data, clusters, labels, n_terms):\n",
    "    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n",
    "    \n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n",
    "            \n",
    "get_top_keywords(text, clusters, tfidf.get_feature_names(), 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
