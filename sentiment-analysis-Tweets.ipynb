{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from pandas) (1.18.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from matplotlib) (1.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (46.1.1.post20200323)\n",
      "Requirement already satisfied: numpy in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (1.18.2)\n",
      "Requirement already satisfied: tweepy in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tweepy) (2.23.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tweepy) (1.7.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tweepy) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from requests>=2.11.1->tweepy) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from requests>=2.11.1->tweepy) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from requests>=2.11.1->tweepy) (1.25.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from nltk) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (2.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from networkx) (4.4.2)\n",
      "Requirement already satisfied: textblob in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from nltk>=3.1->textblob) (1.14.0)\n",
      "Requirement already satisfied: tensorflow==1.15 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (1.15.0)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (1.15.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (1.15.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (3.2.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (3.11.3)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (0.8.1)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (0.9.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (1.18.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow==1.15) (1.27.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (46.1.1.post20200323)\n",
      "Requirement already satisfied: h5py in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from scikit-learn) (1.18.2)\n",
      "Requirement already satisfied: tensorflow_hub in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow_hub) (1.18.2)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow_hub) (3.11.3)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from tensorflow_hub) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from protobuf>=3.4.0->tensorflow_hub) (46.1.1.post20200323)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textwrap3 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (4.5.4)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: six in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (from plotly) (1.14.0)\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\hchau\\anaconda3\\envs\\new_test\\lib\\site-packages (3.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install tweepy\n",
    "!pip install nltk\n",
    "!pip install networkx\n",
    "!pip install textblob\n",
    "!pip install tensorflow==1.15\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow_hub\n",
    "!pip install textwrap3\n",
    "!pip install plotly\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import collections\n",
    "import tweepy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "import textwrap\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import tools\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "# from utils.text_wapper import remove_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config data\n",
    "inputFolder = \"./twitter-data-geo-output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweetsText = []\n",
    "# for file in os.listdir(inputFolder):\n",
    "#     filePath = \"{}/{}\".format(inputFolder, file)\n",
    "#     data = pd.read_json(filePath, orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "# pic_pattern = re.compile('pic\\.twitter\\.com/.{10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the data and create a dict with tweets of a user\n",
    "\n",
    "tweetsText = []\n",
    "\n",
    "user_tweets = defaultdict(str)\n",
    "for file in os.listdir(inputFolder):\n",
    "    filePath = \"{}/{}\".format(inputFolder, file)\n",
    "    with open(filePath, \"r\") as jsonFile:\n",
    "        jsf = json.load(jsonFile)\n",
    "        for j in jsf:\n",
    "            if j['lang'] == 'en':\n",
    "                text = j['text']\n",
    "                text = re.sub(r'http\\S+', '', text)\n",
    "#                 re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "                text.replace('@ ', '@')\n",
    "                text.replace('# ', '#')\n",
    "                tweetsText.append(text)\n",
    "                user_tweets[j['user']['id']] += text\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hchau\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hchau\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Get word embedding mod\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "[_Derived_]  Blas GEMM launch failed : a.shape=(2691, 320), b.shape=(320, 320), m=2691, n=320, k=320\n\t [[{{node EncoderDNN/DNN/ResidualHidden_0/dense/MatMul}}]]\n\t [[StatefulPartitionedCall_1/StatefulPartitionedCall/StatefulPartitionedCall]]\n\t [[StatefulPartitionedCall_1/_379]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: [_Derived_]{{function_node __inference_pruned_2016}} {{function_node __inference_pruned_2016}} Blas GEMM launch failed : a.shape=(2691, 320), b.shape=(320, 320), m=2691, n=320, k=320\n\t [[{{node EncoderDNN/DNN/ResidualHidden_0/dense/MatMul}}]]\n\t [[StatefulPartitionedCall_1/StatefulPartitionedCall/StatefulPartitionedCall]]\n\t [[StatefulPartitionedCall_1/_379]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a7fbdd91d415>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmessage_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\new_test\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1384\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: [_Derived_]  Blas GEMM launch failed : a.shape=(2691, 320), b.shape=(320, 320), m=2691, n=320, k=320\n\t [[{{node EncoderDNN/DNN/ResidualHidden_0/dense/MatMul}}]]\n\t [[StatefulPartitionedCall_1/StatefulPartitionedCall/StatefulPartitionedCall]]\n\t [[StatefulPartitionedCall_1/_379]]"
     ]
    }
   ],
   "source": [
    "# Get word embeddings\n",
    "cfg = tf.ConfigProto(allow_soft_placement=True )\n",
    "cfg.gpu_options.allow_growth = True\n",
    "with tf.Session(config=cfg) as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(list(user_tweets.values())))\n",
    "print(message_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert\n",
    "\n",
    "# from bert_serving.client import BertClient\n",
    "# client = BertClient()\n",
    "# vectors = client.encode(user_tweets_text)\n",
    "# print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster using word embeddings\n",
    "X = message_embeddings\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42).fit(message_embeddings)\n",
    "\n",
    "df = pd.DataFrame(zip(list(user_tweets.keys()), list(user_tweets.values()), kmeans.labels_), columns = ['userid', 'text', 'cluster_text'])\n",
    "\n",
    "df.head()\n",
    "\n",
    "# group = defaultdict(list)\n",
    "# for index, text in enumerate(user_tweets_text):\n",
    "#     group[kmeans.labels_[index]].append(text)\n",
    "    \n",
    "# for cluster_index in group:\n",
    "#     print(group[cluster_index])\n",
    "#     print(\"-----------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Cosine Similality to each Cluster Center\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2)/(np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "norm = np.array(np.zeros(len(X)*5).reshape(len(X), 5))\n",
    "for i in range(len(X)):\n",
    "    for j in range(5):\n",
    "        norm[i, j] = cos_sim(X[i], kmeans.cluster_centers_[j])\n",
    "        \n",
    "grp_df = pd.concat([df,\n",
    "                    pd.DataFrame(norm, index=df.index).rename(columns={0:'sim0', 1:'sim1', 2:'sim2', 3:'sim3', 4:'sim4'})],\n",
    "                    axis=1)\n",
    "# grp_df = pd.DataFrame(norm, message_embeddings).rename(columns={0:'sim0', 1:'sim1', 2:'sim2', 3:'sim3', 4:'sim4'})\n",
    "# df = pd.DataFrame(list(zip(norm[:], message_embeddings)), columns = ['sim0','sim1','sim2','sim3','sim4', 'text'])\n",
    "\n",
    "# grp_df = pd.concat([data_president2,\n",
    "#                     pd.DataFrame(norm, index=data_president2.index).rename(columns={0:'sim0', 1:'sim1', 2:'sim2', 3:'sim3'})],\n",
    "#                     axis=1)\n",
    "\n",
    "# Divide tweets data by Cluster\n",
    "df_0 = grp_df[grp_df['cluster_text']==0].sort_values('sim0', ascending=False)\n",
    "df_1 = grp_df[grp_df['cluster_text']==1].sort_values('sim1', ascending=False)\n",
    "df_2 = grp_df[grp_df['cluster_text']==2].sort_values('sim2', ascending=False)\n",
    "df_3 = grp_df[grp_df['cluster_text']==3].sort_values('sim3', ascending=False)\n",
    "df_4 = grp_df[grp_df['cluster_text']==4].sort_values('sim3', ascending=False)\n",
    "\n",
    "grp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    analysis = TextBlob(text)\n",
    "    Sentiment = analysis.sentiment[0]\n",
    "    return Sentiment\n",
    " \n",
    "\n",
    "def subjectivity_analysis(text):\n",
    "    analysis = TextBlob(text)\n",
    "    Subjectivity = analysis.sentiment[1]\n",
    "    return Subjectivity \n",
    "\n",
    "df[\"Sentiment\"] = df.text.map(lambda x:sentiment_analysis(x) )\n",
    "df[\"Subjectivity\"] = df.text.map(lambda x:subjectivity_analysis(x) )\n",
    "\n",
    "# df[[\"Sentiment\",\"Subjectivity\"]].head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def typical_tweets(df):\n",
    "    for i in range(5):\n",
    "        wrap_list = textwrap.wrap(df.iloc[i,1], 76)\n",
    "        print('\\n'.join(wrap_list),'\\n')\n",
    "        \n",
    "typical_tweets(df_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typical_tweets(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typical_tweets(df_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typical_tweets(df_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typical_tweets(df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['coronavirus', 'COVID', 'virus', 'corona', 'pandemic', 'quarantine']\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def is_relevant(tweet):\n",
    "    word_tokens = tokenizer.tokenize(tweet)\n",
    "    word_tokens = [word for word in word_tokens if word not in stop_words]\n",
    "   # word_tokens = [ps.stem(w) for w in word_tokens]\n",
    "    result =  any(elem in word_tokens for elem in keywords)\n",
    "    return result\n",
    "\n",
    "relevant_tweets = [tweet for tweet in tweetsText if is_relevant(tweet)]\n",
    "\n",
    "print(len(relevant_tweets))\n",
    "print(len(tweetsText))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweet in tweetsText:\n",
    "#     if is_relevant(tweet):   \n",
    "#         if word in vocab:\n",
    "#             vocab[word].add(doc_id)\n",
    "#         else:\n",
    "#             vocab[word] = set([doc_id])\n",
    "#     return vocab      \n",
    "            \n",
    "# vocab = parser()\n",
    "# print(len(vocab))\n",
    "# tweetsText = [tweet for tweet in tweetsText if keywords in tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_objects = [TextBlob(tweet) for tweet in tweetsText]\n",
    "# sentiment_objects[0].polarity, sentiment_objects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis in each cluster grouped by word embeddings\n",
    "\n",
    "for i in range(0,5):\n",
    "    df_grp = df[df['cluster_text']==i]\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    df_grp[[\"Sentiment\", \"Subjectivity\"]].hist(bins=[-1, -0.75, -0.5, -0.25, 0.25, 0.5, 0.75, 1],\n",
    "             ax=ax,\n",
    "             color=\"purple\")\n",
    "#     plt.title(\"Sentiments from recent Tweets during the COVID period\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# group_count = len(group)\n",
    "# for i in range(0,group_count):\n",
    "#     sentiment_objects = [TextBlob(tweet) for tweet in group[i]]\n",
    "#     sentiment_values = [[tweet.sentiment.polarity, tweet.sentiment.subjectivity, str(tweet)] for tweet in sentiment_objects]\n",
    "# #     sentiment_df = pd.DataFrame(scores)\n",
    "#     sentiment_df = pd.DataFrame(sentiment_values, columns=[\"polarity\", \"subjectivity\", \"tweet\"])\n",
    "#     sentiment_df.head()\n",
    "#     print(\"\\n\\n\")\n",
    "#     fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# # Plot histogram of the polarity values\n",
    "#     sentiment_df.hist(bins=[-1, -0.75, -0.5, -0.25, 0.25, 0.5, 0.75, 1],\n",
    "#              ax=ax,\n",
    "#              color=\"purple\")\n",
    "\n",
    "#     plt.title(\"Sentiments from recent Tweets during the COVID period\")\n",
    "#     plt.show()\n",
    "    \n",
    "# #     print(sentiment_df[(sentiment_df.polarity > 0.75)])\n",
    "# #     print(sentiment_df[(sentiment_df.polarity < -0.5)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly\n",
    "\n",
    "# px.scatter(sentiment_df.sample(n=3000,  replace=True), x= \"polarity\", y = \"subjectivity\")\n",
    "fig = px.scatter(df, x=df.Sentiment, y=df.Subjectivity, color=df.cluster_text)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.head())\n",
    "df_kmeans = df[[\"Sentiment\",\"Subjectivity\"]]\n",
    "df_kmeans.head()\n",
    "tweets_array = np.array(df_kmeans)\n",
    "# px.scatter(df, x=\"Sentiment\", y=\"Subjectivity\",color=\"cluster_sentiment\")\n",
    "n_cluster = 5\n",
    "pred = KMeans(n_clusters=n_cluster).fit_predict(tweets_array)\n",
    "df[\"cluster_sentiment_TextBlob\"] = pred\n",
    "df.head()\n",
    "px.scatter(df, x=\"Sentiment\", y=\"Subjectivity\",color=\"cluster_sentiment_TextBlob\", hover_data=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Using Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vaderSentiment\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores =[]\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "    \n",
    "for sentence in list(user_tweets.values()):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    scores.append(score)\n",
    "\n",
    "# df[\"Sentiment\"] = df.text.map(lambda x:sentiment_analysis(x) )\n",
    "dataFrame= pd.DataFrame(scores)\n",
    "\n",
    "print(dataFrame)\n",
    "\n",
    "print(\"Overall Sentiment Score for the multiple sentences :- \",dataFrame.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
